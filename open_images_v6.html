<!DOCTYPE html>
<html>
    <head>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>
        <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" rel="stylesheet"></link>
    </head>
    <body>
    <nav class="navbar navbar-dark bg-dark">
        <a class="navbar-brand" href="main.html">FMI Data Index</a>
        <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarsExample01" aria-controls="navbarsExample01" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="navbar-collapse collapse" id="navbarsExample01" style="">
            <ul class="navbar-nav mr-auto">
                <li class="nav-item">
                    <a class="nav-link" href="cv_list.html">CV</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="nlp_list.html">NLP</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="audio_list.html">Audio</a>
                </li>
            </ul>
        </div>
    </nav>
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
        integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"
        integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1"
        crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"
        integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
        crossorigin="anonymous"></script>       
    <div class='container' style="margin-top:2em;">
        <center><h1>Open Images V6</h1></center>
        <center><a href="https://storage.googleapis.com/openimages/web/index.html?v6" target="_blank">https://storage.googleapis.com/openimages/web/index.html?v6</a></center>
    </div>
    <div class="container" style="margin-top:2em;">
        <div class="col" style="background:rgb(64, 147, 214);">
            <h2><font color='white'>Overview</font></h2>
        </div>
        <div class="col">
            <p>Open Images is a dataset of around 9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, visual relationships, and localized narratives. The dataset contains:</p>
            <ul>
                <li>16 million bounding boxes for 600 object classes on 1.9 million images, making it the largest existing dataset with object location annotations;</li>
                <li>over 2.7 million instance segmentation on 350 categories;</li>
                <li>over 3.2 million relationship annotations on 1446 relationships;</li>
                <li>over 500 000 localized narratives;</li>
                <li>over 59 million image-level labels on over 19 000 categories.</li>
            </ul>
             
            <p>The boxes have been largely manually drawn by professional annotators to ensure accuracy and consistency. The images are very diverse and often contain complex scenes with several objects (8.3 per image on average). Open Images also offers visual relationship annotations, indicating pairs of objects in particular relations (e.g. "woman playing guitar", "beer on table"), object properties (e.g. "table is wooden"), and human actions (e.g. "woman is jumping").</p>
            <p>The authors believe that having a single dataset with unified annotations for image classification, object detection, visual relationship detection, instance segmentation, and multimodal image descriptions will enable to study these tasks jointly and stimulate progress towards genuine scene understanding.</p>
            <p>The extended version of the dataset contians an additional 478 000 crowdsourced images with 6000 categories.</p>
        </div>
    </div>
    <div class="container" style="margin-top:2em;">
        <div class="col" style="background:rgb(64, 147, 214);">
            <h2><font color='white'>Associated Paper or Article</font></h2>
        </div>
        <div class="col">
            <p>For more information on images themselves, please read <a href="https://arxiv.org/abs/1811.00982" target="_blank">The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale</a>.</p>    
            <p>For more information on the annotation methodology, please read <a href="https://arxiv.org/pdf/1903.10830.pdf" target="_blank">Large-scale interactive object segmentation with human annotators</a>.</p>
            <p>For more information on local narratives, please read <a href="https://arxiv.org/abs/1912.03098" target="_blank">Connecting Vision and Language with Localized Narratives</a>.</p>
        </div>
    </div>
    <div class="container" style="margin-top:2em;">
        <div class="col" style="background:rgb(64, 147, 214);">
            <h2><font color='white'>Annotations</font></h2>
        </div>
        <div class="col">
            <p>1. The bounding box annotations have the following structure:</p>
            <img src="./static/images/oiv6_bbox.png" width="1000px">
            <p>2. The image segmentation masks are black and white images corresponding to the training images, in which the object appears as white.</p>
            <p>3. The relationship annotations have the following structure:</p>
            <img src="./static/images/oiv6_rel.png" width="1000px">
            <p>4. The localized narratives have the following structure:</p>
            <img src="./static/images/oiv6_locnar.png" height="200px">    
        </div>
    </div>
    <div class="container" style="margin-top:2em;">
        <div class="col" style="background:rgb(64, 147, 214);">
            <h2><font color='white'>Download</font></h2>
        </div>
        <div class="col">
            <p>You can download the dataset <a href="https://storage.googleapis.com/openimages/web/download.html" target="_blank">here</a>.</p>
        </div>
    </div>
    <div class="container" style="margin-top:2em;">
        <div class="col" style="background:rgb(64, 147, 214);">
            <h2><font color='white'>Model</font></h2>
        </div>
        <div class="col">
            <p>No official model has been provided for this dataset. However, you can check the models from the <a href="https://storage.googleapis.com/openimages/web/challenge.html" target="_blank">2018</a> and <a href="https://storage.googleapis.com/openimages/web/challenge2019.html" target="_blank">2019</a> competition leaderboards.</p>
        </div>
    </div>
    <div class="container" style="margin-top:2em;">
        <div class="col" style="background:rgb(64, 147, 214);">
            <h2><font color='white'>Benchmarks</font></h2>
        </div>
        <div class="col">
            <p>No official benchmark has been provided for this dataset. However, you can check the leaderboards from the <a href="https://storage.googleapis.com/openimages/web/challenge.html" target="_blank">2018</a> and <a href="https://storage.googleapis.com/openimages/web/challenge2019.html" target="_blank">2019</a> competitions.</p>
        </div>
    </div>
    <div class="container" style="margin-top:2em;">
        <div class="col" style="background:rgb(64, 147, 214);">
            <h2><font color='white'>Associated Challenges</font></h2>
        </div>
        <div class="col">
            <p>There are several challenges associated with this dataset: <a href="https://storage.googleapis.com/openimages/web/challenge.html" target="_blank">Open Images Challenge 2018</a>, <a href="https://storage.googleapis.com/openimages/web/challenge2019.html" target="_blank">Open Images Challenge 2019</a> and <a href="http://www.robustvision.net/" target="_blank">Robust Vision Challenge 2020</a>.</p>
            <p>The challenges focus on object detection, visual relationship detection and instance segmentation.</p>
        </div>
    </div>
    <div class="container" style="margin-top:2em;">
        <div class="col" style="background:rgb(64, 147, 214);">
            <h2><font color='white'>License</font></h2>
        </div>
        <div class="col">
            <p>Dataset licensed under the CC BY 4.0 license.</p>
        </div>
    </div>
    </body>
</html>