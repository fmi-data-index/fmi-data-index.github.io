<!DOCTYPE html>
<html>
    <head>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>
        <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" rel="stylesheet"></link>
    </head>
    <body>
    <nav class="navbar navbar-dark bg-dark">
            <a class="navbar-brand" href="main.html">FMI Data Index</a>
            <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarsExample01" aria-controls="navbarsExample01" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="navbar-collapse collapse" id="navbarsExample01" style="">
                <ul class="navbar-nav mr-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="cv_list.html">CV</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="nlp_list.html">NLP</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="audio_list.html">Audio</a>
                    </li>
                </ul>
            </div>
        </nav>
        <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
            integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
            crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"
            integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1"
            crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"
            integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
            crossorigin="anonymous"></script>       
        <div class='container' style="margin-top:2em;">
            <center><h1>DensePose: Dense Human Pose Estimation In The Wild</h1></center>
            <center><a href="http://densepose.org/" target="_blank">http://densepose.org/</a></center>
        </div>
        <div class="container" style="margin-top:2em;">
            <div class="col" style="background:rgb(64, 147, 214);">
                <h2><font color='white'>Overview</font></h2>
            </div>
            <div class="col">
                <p>DensePose proposes a set of 50 000 annotations to establish dense correspondences from 2D images to surface-based representations of the human body. If done naively, this would require by manipulating a surface through rotations - which can be frustratingly inefficient. Instead, we construct a two-stage annotation pipeline to efficiently gather annotations for image-to-surface correspondence. </p>
                <p>In the first stage we ask annotators to delineate regions corresponding to visible, semantically defined body parts. We instruct the annotators to estimate the body part behind the clothes, so that for instance wearing a large skirt would not complicate the subsequent annotation of correspondences.</p>

                <p>In the second stage we sample every part region with a set of roughly equidistant points and request the annotators to bring these points in correspondence with the surface. In order to simplify this task we `unfold' the part surface by providing six pre-rendered views of the same body part and allow the user to place landmarks on any of them. This allows the annotator to choose the most convenient point of view by selecting one among six options instead of manually rotating the surface.
                    We use the SMPL model and SURREAL textures in the data gathering procedure. </p>
                <div class="text-center">
                    <img src="./static/images/dense1.svg" width="1000" class="img-responsive"/>
                </div>
                <p>Sample annotated image:</p>
                <div class="text-center">
                    <img src="./static/images/dense2.png" width="1000" class="img-responsive"/>
                </div>
            </div>
        </div>
        <div class="container" style="margin-top:2em;">
                <div class="col" style="background:rgb(64, 147, 214);">
                    <h2><font color='white'>Associated Paper or Article</font></h2>
                </div>
                <div class="col">
                    <p>More information can be found by reading <a href="https://arxiv.org/pdf/1802.00434.pdf" target="_blank">DensePose: Dense Human Pose Estimation In The Wild</a>.</p>
                </div>
        </div>
        <div class="container" style="margin-top:2em;">
                <div class="col" style="background:rgb(64, 147, 214);">
                    <h2><font color='white'>Download</font></h2>
                </div>
                <div class="col">
                    As this dataset is part of two challenges, there are two means of downloading the data, either through <a href="https://github.com/facebookresearch/DensePose/blob/master/INSTALL.md#fetch-densepose-data">DensePose-COCO Track</a> or <a href="https://github.com/facebookresearch/DensePose/tree/master/PoseTrack#fetch-densepose-posetrack-dataset">DensePose-Pose Track</a>.
                </div>
        </div>
        <div class="container" style="margin-top:2em;">
                <div class="col" style="background:rgb(64, 147, 214);">
                    <h2><font color='white'>Model</font></h2>
                </div>
                <div class="col">
                    <p>One model is associated with this dataset, and that is the DensePose-RCNN System, as described below:</p>
                    <p>
                        We adopt the architecture of Mask-RCNN with the Feature Pyramid Network (FPN) features, and ROI-Align pooling so as to obtain dense part labels and coordinates within each of the selected regions.
                        As shown below, we introduce a fully-convolutional network on top of the ROI-pooling that is entirely devoted to two tasks:</p>
                        
                    <p>   Generating per-pixel classification results for selection of surface part. For each part regressing local coordinates within part.</p>
                        
                    <p>   During inference, our system operates at 25fps on 320x240 images and 4-5fps on 800x1100 images using a GTX1080 graphics card.</p>
                    <p>
                        The DensePose-RCNN system can be trained directly using the annotated points as supervision. However, we obtain substantially better results by ``inpainting'' the values of the supervision signal on positions that are not originally annotated. To achieve this, we adopt a learning-based approach where we firstly train a ``teacher'' network: A fully-convolutional neural network(depicted below) that reconstructs the ground-truth values given images scale-normalized images and the segmentation masks.
                        </p>
                    <p>
                        We further improve the performance of our system using cascading strategies. Via cascadning, we exploit information from related tasks, such as keypoint estimation and instance segmentation, which have successfully been addressed by the Mask-RCNN architecture. This allows us to exploit task synergies and the complementary merits of different sources of supervision.
                        </p>
                </div>
        </div>
        <div class="container" style="margin-top:2em;">
                <div class="col" style="background:rgb(64, 147, 214);">
                    <h2><font color='white'>Benchmarks</font></h2>
                </div>
                <div class="col">
                    No benchmarks have been provided for this dataset.
                </div>
        </div>
        <div class="container" style="margin-top:2em;">
                <div class="col" style="background:rgb(64, 147, 214);">
                    <h2><font color='white'>Associated Challenges</font></h2>
                </div>
                <div class="col">
                    <p>This dataset was associated to two challenges, on <a href="http://cocodataset.org/workshop/coco-mapillary-iccv-2019.html">COCO and Mapillary Joint Recognition Challenge Workshop at ICCV 2019</a>.
                </div>
        </div>
        <div class="container" style="margin-top:2em;">
            <div class="col" style="background:rgb(64, 147, 214);">
                <h2><font color='white'>Licence</font></h2>
            </div>
            <div class="col">
                <p>The dataset is lincenced under the <a href="https://creativecommons.org/licenses/by-nc/2.0/"> CC BY-NC 2.0</a> licence.</p>
            </div>
        </div>
    </body>
</html>